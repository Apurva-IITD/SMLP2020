---
title: "Reading children"
author: "Anastasiya Lopukhina"
date: "2020-09-09"
---

**rk comments**

# Setup

Packages we (might) use.

```julia

using DrWatson
@quickactivate "SMLP2020"

using MixedModels
using DataFrames, DataFramesMeta, RCall 
using Statistics: mean
```

```julia
R"""
require("lme4", quietly=TRUE)
require("remef")
require("tidyverse", quietly=TRUE)
require("broom.mixed", quietly=TRUE)
require("performance", quietly=TRUE)
require("grid", quietly=TRUE)
require("gridExtra", quietly=TRUE)
source($srcdir('LMM_residuals.R'))
""";

RCall.ijulia_setdevice(MIME("image/svg+xml"), width=6, height=3.5)
```

We read the data preprocessed with R and saved as RDS file (see `Lopukhina_dyslx.Rmd`).

```julia
dat = DataFrame(rcopy(R"readRDS($datadir('Lopukhina_ffd.RDS'))"));
describe(dat)
```

**DV**
log(ffd):  log of first fixation duration

**Predictors**

+ f: log frequency of word
+ l: length of word
+ grd: grade level?
+ snd: sound

**Control**

+ ao: amplitude (outgoing)  
+ lp: landing position (IOVP; should this we quadratic?)  
+ l1: length of last word
+ l2: length of next word
+ rpw: relative position of word in sentence

**Questions* 

+ Why not lauchsite?
+ ao:  needs to be checked 

**Group**

Non-dyslexic children serve as control group for dyslexic children (treamtent/dummy contrast).

```julia
dat = @transform(dat, Group = levels!(:Group, ["norm", "dyslexia"]))
```

## LMM with lme4

AL's `oviLMM`

```julia
@rput dat;

R"""
ffd_m1 <- lmer(log(ffd) ~ 1 + f + l + grd + snd + 
                         lp + l1 + l2 + rpw +
                         (1 | Subj ) + (1 | Item) + (1 | Word), 
                data=dat,  REML = FALSE,
                control = lmerControl(calc.derivs=FALSE))

check_model(ffd_m1, check=c('qq', 'reqq'))
#plot_LMM_residuals(ffd_m1)

#plot(fitted(ffd_m1), residuals(ffd_m1))
#qqnorm(residuals(ffd_m1))
"""
```

Adding "Group", using "norm" as reference for "dyslexia"

```julia
R"""
contrasts(dat$Group) <- contr.treatment(2, base=2)
mm <- model.matrix( ~ 1 + Group, data=dat)
dat$grp <-  mm[,2]
table(dat$Group, dat$grp)

ffd_m2 <- lmer(log(ffd) ~ 1 + Group*(f + l + grd + snd + 
                          lp + l1 + l2 + rpw) +
                         (1 | Subj ) + (1 | Item) + (1 | Word), 
                data = dat,  REML = FALSE,
                control = lmerControl(calc.derivs=FALSE))

anova(ffd_m1, ffd_m2)
"""
```

# LMM with `MixedModels`

## Contrasts

```julia
cntrsts = merge(
    Dict(:Group => DummyCoding()),
    Dict(index => Grouping() for index in (:Subj, :Word, :Item,)),
);
```

## AL's LMM

```julia
f1 = @formula (log(ffd))  ~ 1 + f + l + grd + snd + lp + l1 + l2 + rpw +
                           (1 | Subj ) + (1 | Item) + (1 | Word);
m1 = fit(MixedModel, f1, dat, contrasts=cntrsts);
```

## Adding `Group` and its interaction with covariates

```julia
f2 = @formula (log(ffd))  ~ 1 + Group*(f + l + grd + snd + lp + l1 + l2 + rpw) +
                           (1 | Subj ) + (1 | Item) + (1 | Word);
m2 = fit(MixedModel, f2, dat, contrasts=cntrsts);

MixedModels.likelihoodratiotest(m2, m1)
show(m2)
```

## Adding VPs

Checking individual and item differences in `l1`, `ao`, and  `rpw` -- also their interaction with `Group`. Now we need to engage in model selection. 

```julia
f3 = @formula (log(ffd))  ~ 1 + Group*(f + l + grd + snd + lp + l1 + l2 + rpw) + 
               zerocorr(1 + lp + l1 + l2 + rpw + f + l + grd + snd         | Subj) + 
               zerocorr(1 + lp + l1 + l2 + rpw                     + Group | Item) + 
               zerocorr(1 + lp + l1 + l2 + rpw                     + Group | Word);
m3 = fit(MixedModel, f3, dat, contrasts=cntrsts);
VarCorr(m3)
m3.PCA

# Save LMM
using Serialization
serialize(projectdir("fits", "m3.jls"), m3) 
# Retrieve LMM
#deserialize(projectdir("fits", "m3.jls"));
```

There are a many very small VCs; the model is seriously overparameterized. The PCA suggests:

+ Subj: 9
+ Word: 5
+ Item: 3 

I take out VCs estimated at zero. 

```julia
f4 = @formula (log(ffd))  ~ 1 + Group*(grd + snd + lp + l1 + l2 + rpw) +
               zerocorr(1 + lp + l1 + rpw + f + l + grd + snd         | Subj) + 
               zerocorr(1           + rpw                     + Group | Item) + 
               zerocorr(1 + lp + l1 + rpw                             | Word);

m4 = fit(MixedModel, f4, dat, contrasts=cntrsts);
VarCorr(m4)
m4.PCA

MixedModels.likelihoodratiotest(m4, m3)

# save 
serialize(projectdir("fits", "m4.jls"), m4) 

```

LMM `m4` is supported by the data. So let's add the CPs.

```julia
f5 = @formula (log(ffd)) ~ 1 + Group*(grd + snd + lp + l1 + l2 + rpw) +
                          (1 + lp + l1 + rpw + f + l + grd + snd        | Subj) + 
                          (1           + rpw                    + Group | Item) + 
                          (1 + lp + l1 + rpw                            | Word);

m5 = fit(MixedModel, f5, dat, contrasts=cntrsts);
VarCorr(m5)
m5.PCA

MixedModels.likelihoodratiotest(m5, m4, m3)

show(m4)
show(m5)

# save
serialize(projectdir("fits", "m5.jls"), m5) 
```



## Ship fitted model to R for postprocessing

```julia
m5b = Tuple([m5, dat]);
@rput m5b;
```

Well, not all of them yet ...

## Model diagnostics

```julia

```

# Appendix 

## Weave the document in the REPL

```
julia> using Weave
julia> weave(scriptsdir("Lopukhina_dyslx_rk.jmd"), doctype="md2html")
```

## Switch to jupyter notebook from REPL

```
julia> using Weave, IJulia
julia> convert_doc(scriptsdir("Lopukhina_dyslx_rk.jmd"), projectdir("notebooks","Lopukhina_dyslx_rk.ipynb"))
julia> IJulia.notebook(dir=projectdir("notebooks"))
```

## Info

```julia
using InteractiveUtils
versioninfo()
```
