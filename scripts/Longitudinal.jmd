
# Longitudinal data - sleepstudy

## Preliminaries

Load the packages to be used

```julia
using DrWatson
@quickactivate "SMLP2020"
using DataFrames, JellyMe4, MixedModels, RCall
```

## Repeated measures and longitudinal data

The random effects in a mixed-effects model are always associated with the levels of a grouping factor, which are the experimental or observational units.
It only makes sense to use random effects when there are more than one observations for (at least some of) these units.
We say that there are _repeated measures_.

Frequently the grouping factor is _subject_, as in the experiment described below.

If the repeated measures are taken over time we say they are _longitudinal data_.
We wish to characterize the response over time within subjects and
the variation in the time trends between subjects.
Often we are not as interested in comparing the
particular subjects in the study as much as we are interested in
modeling the variability in the population from which the subjects
were chosen.

## Data from a study on sleep deprivation

Belenky et al. (2003) conducted an experiment to measure the 
effect of sleep deprivation on cognitive performance.
There were 18 subjects, chosen from the population of interest
(long-distance truck drivers), in the 10 day trial. These subjects were  restricted to 3 hours sleep per night during the trial.

On each day of the trial each subject's reaction time was measured.
The reaction time shown here is the average of several measurements.

These data are *balanced* in that each subject is measured the same number of times and on the same occasions.

Load the data

```julia
sleepstudy = MixedModels.dataset(:sleepstudy);
describe(sleepstudy)
```

and plot the data using the _lattice_ package in R

Load the `lattice` package in `R` and set the graphics device to SVG (scalable vector graphics).

```julia
R"""
require("ggplot2", quietly=TRUE)
require("lattice", quietly=TRUE)
require("lme4", quietly=TRUE)
""";
RCall.ijulia_setdevice(MIME("image/svg+xml"), width=6, height=3.5)
```

Transfer the Julia `DataFrame` to an R `data.frame` with the same name

```julia
@rput sleepstudy;
```

and produce the plot

```julia
R"""
xyplot(reaction ~ days | subj, sleepstudy, aspect = "xy",
    layout = c(9,2), type = c("g", "p", "r"),
    index.cond = function(x,y) coef(lm(y ~ x))[1],
    xlab = "Days of sleep deprivation",
    ylab = "Average reaction time (ms)")
"""
```

## Some comments on the data plot

The plot is a _lattice_ plot where the data for each subject are presented in a separate panel.
The axes are consistent across panels so we may compare patterns across subjects.

A reference line fit by simple linear regression to the panel's data has been added to each panel.  Also, the aspect ratio of the panels has been adjusted so that a typical reference line lies about $45^\circ$ on the page.
We have the greatest sensitivity in checking for differences in slopes when the lines are near $\pm 45^\circ$ on the page.

Importantly, the panels have been ordered not by subject number (which is essentially a random order) but according to increasing intercept for the simple linear regression.
If the slopes and the intercepts are highly correlated we should see a pattern across the panels in the slopes.

Before commenting on the within-subject linear regressions we will show how to evaluate the coefficients of those regression models using the Julia version of _split-apply-combine_.

## The _groupby-do-combine_ strategy in Julia

Hadley Wickham wrote about a _split-apply-combine_ strategy of working with subsets of data in a data frame based on the levels of a factor in the data.
There are several methods in the _DataFrames_ package for Julia to implement a similar strategy.

In the R formulation the `apply` part of the name comes from the names of the group of functions used for _functional programming_.
You `apply` a function to elements of a list or other compound structure producing a new compound structure.

We will show one way of performing this kind of functional programming in Julia using a _do-block_, which is a way of writing an anonymous function.

The `split` part is done by a function called `groupby`.
The first argument is the data frame and the second argument is the name of a column or a vector of column names as _symbols_.
A symbol is written in Julia with a colon followed by the name.

The general pattern is
```
sumry = combine(
    groupby(df, factor_name)) do sdf
        # operate on columns of the subDataFrame
    end
```

The statements between `do` and `end` are the body of the function to apply to the subDataFrame `sdf`.  This anonymous function should end in creating a `Tuple` of name/value pairs.

Suppose we wish to evaluate the coefficients of the within-subject regressions.
For each subject the `days` column is the `x` in the regression and the `reaction` column is the `y`.
We create the model matrix as, e.g.

```julia
let days = 0:9
    [ones(length(days)) days]
end
```

and use the backslash operator to evaluate the coefficients.

Putting this together looks like

```julia
withinsubj = combine(groupby(sleepstudy, :subj)) do sdf
    X = [ones(length(sdf.days)) sdf.days]
    coefs = X \ sdf.reaction
    (intercept = first(coefs), slope = last(coefs), )
end
```

The advantage of using this formulation is that it is easy to extend the quantities being evaluated and returned.
Suppose we wish to return the sum of squared residuals in addition to the coefficient estimates.

```julia
withinsubj = combine(groupby(sleepstudy, :subj)) do sdf
    X = [ones(length(sdf.days)) sdf.days]
    coefs = X \ sdf.reaction
    dfr = length(sdf.days) - 2  # degrees of freedom for residuals
    ssr = sum(abs2, sdf.reaction - X * coefs) # sum of squared res
    (intercept = first(coefs), slope = last(coefs),
    ssr = ssr, dfr = dfr, s = sqrt(ssr / dfr), )
end
```

```julia
describe(withinsubj)
```

### Assessing the within-subject linear regressions

In most cases a simple linear regression provides an adequate fit to the within-subject data. 
Patterns for some subjects (e.g. 350, 352 and 371) deviate from linearity but the deviations are neither widespread nor consistent in form.

There is considerable variation in the intercept (estimated reaction time without sleep deprivation) across subjects -- 200 ms. up to 300 ms. -- and in the slope (increase in reaction time per day of sleep deprivation) -- 0 ms./day up to 20 ms./day.
Also the quality of the fit as measured by `s`, the estimate of `σ`, which ranges from about 9 to over 60 ms.

## Fit a linear mixed-effects model

Begin with a linear mixed-effects model with fixed effects for intercept and slope (w.r.t. `days`) and by-subject random effects for intercept and slope.
By default the slope and intercept random effects are correlated within subject.

```julia
f1 =  @formula(reaction ~ 1 + days + (1+days|subj));
m1 = fit(MixedModel, f1, sleepstudy)
```

The random-effects "estimates" (technically, these are the conditional means of the random effects given the observed data) can be obtained as

```julia
ranefvals = DataFrame(only(raneftables(m1)))
```

To put these on the same scale as the by-subject regression coefficients we add in the fixed-effects estimates.

```julia
let fe = fixef(m1)
    ranefvals[2] .+= fe[1]
    ranefvals[3] .+= fe[2]
end;
describe(ranefvals)
```

and combine these values with the within-subject estimates

```julia
coefs = innerjoin(ranefvals, withinsubj, on = :subj);
@rput coefs
```

A scatter plot of the within-subject estimates versus the predictions from the mixed-effects model shows shrinkage toward the population mean.

```julia
RCall.ijulia_setdevice(MIME("image/svg+xml"), width=6, height=5)
R"""
p <- ggplot(coefs, aes(slope, intercept)) +
    geom_point(aes(color="Within")) +
    geom_text(aes(label=subj), vjust="outward") +
    geom_point(aes(days, `(Intercept)`, color="Mixed")) +
    geom_segment(aes(xend=days, yend=`(Intercept)`),
        arrow = arrow(length=unit(0.01, "npc")))
"""
```

John Tukey characterized the shrinkage as "borrowing strength".
Our basic assumption is that the subjects are a random sample from the population of interest so it would make sense that they have some characteristics in common.
The value of the slope and intercept from the mixed-effects model for a particular subject is a compromise between the population values (i.e. the fixed effects estimates) and the within-subject estimate.

However, there is more to the story than simply shrinking within-subject estimates that are far from the population values toward the population values.
Consider the shrinkage for `S330`, `S331` and `S337` whose within-subject intercept estimates are among the largest.
This means that their panels in the original data plot are at the top-right and their arrows in the shrinkage plot are along the top.
The mixed-effects estimates for subjects `S330` and `S331` are shrunk considerably toward the population values whereas those for `S337` are not.

The original data plot shows that the within-subject linear model fit for `S337` is much better than those for `S330` and `S331`.
We can also see this from the sum of squared residuals which is 2131 for `S337` and nearly twice as large for `S330` and `S331`.

```julia
coefs[map(∈(("S330","S331","S337")), coefs.subj), :]
```

## Simplifying the random-effects structure

The estimate of the within-subject correlation of the random effects for slope and intercept is 0.08, indicating that it may be reasonable to model the random-effects as independent.
We can fit such a model as

```julia
f2 = @formula(reaction ~ 1 + days + zerocorr(1+days|subj));
m2 = fit(MixedModel, f2, sleepstudy)
```

Because model `m2` is nested within `m1`, which means that it is a special case of `m1`, we can use a likelihood ratio test to compare them.

```julia
MixedModels.likelihoodratiotest(m2, m1)
```

The large p-value indicates that `m1` is not a substantially better fit than `m2` so we adopt the simpler `m2`.
Also the values of AIC, BIC and AICc, where smaller is better, all prefer `m2`.

```julia
DataFrame(map(m -> (objective = objective(m), AIC = aic(m), AICc = aicc(m), BIC = bic(m)), [m2, m1]))
```

## Properties

There are many quantities of interest that can be extracted from a model.
For example, in the previous code block we used an extractor function `objective` to obtain negative twice the log-likelihood of the model, which is the objective optimized in the model-fitting process.
(This is on the scale of the deviance, which is the name used in the `likelihoodratiotest` table.
Technically this is not the deviance because there is a constant that should be added to this value.
However, differences in the objective for models being compared do correspond to differences in the deviance because the constant cancels out in the subtraction.)

The objective is also available as a "properties" of the model using a '.' after the model's name.

```julia
m2.objective
```

To determine the properties that can be extracted in this way, use `propertynames`.

```julia
propertynames(m1)
```

## Creating an lmer model from a `LinearMixedModel`

Sometimes it is convenient to use a system that you already know rather than trying to learn a whole new system simply to perform one task.
Phillip Alday has created the `JellyMe4` package to allow you to create an `lmer` model from a `LinearMixedModel`.
Because the `lme4` package stores a `model.frame` as part of a linear mixed-effects model object, a Tuple of the model and the data frame must be passed to R.

```julia
m1_sleep = (m1, sleepstudy);
@rput m1_sleep;
```

```julia
R"summary(m1_sleep)"
```

```julia
```
